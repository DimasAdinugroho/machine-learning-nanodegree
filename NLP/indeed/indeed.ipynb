{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "sys.append('.')    \n",
    "sys.append('..')    \n",
    "\n",
    "from \n",
    "# use gensim/word2vec/tf-idf to change words to vectors\n",
    "# use multi-label (scikit-multilearn or scikit-learn)\n",
    "\n",
    "def read_data(filename):\n",
    "    if \"csv\" in filename.lower():\n",
    "        return pd.read_table(filename, header=0, sep=sep)\n",
    "    elif \"xls\" in filename.lower():\n",
    "        return pd.read_excel(filename)\n",
    "    elif \"tsv\" in filename.lower():\n",
    "        return pd.read_table(filename, header=0, sep='\\t')\n",
    "    \n",
    "df = read_data('train.tsv')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>licence-needed supervising-job 5-plus-years-ex...</td>\n",
       "      <td>THE COMPANY    Employer is a midstream service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2-4-years-experience-needed salary full-time-job</td>\n",
       "      <td>ICR Staffing is now accepting resumes for Indu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>part-time-job</td>\n",
       "      <td>This is a great position for the right person....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>licence-needed</td>\n",
       "      <td>A large multi-specialty health center is expan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5-plus-years-experience-needed full-time-job b...</td>\n",
       "      <td>JOB PURPOSE:    The Account Director is respon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tags  \\\n",
       "0  licence-needed supervising-job 5-plus-years-ex...   \n",
       "1   2-4-years-experience-needed salary full-time-job   \n",
       "2                                      part-time-job   \n",
       "3                                     licence-needed   \n",
       "4  5-plus-years-experience-needed full-time-job b...   \n",
       "\n",
       "                                         description  \n",
       "0  THE COMPANY    Employer is a midstream service...  \n",
       "1  ICR Staffing is now accepting resumes for Indu...  \n",
       "2  This is a great position for the right person....  \n",
       "3  A large multi-specialty health center is expan...  \n",
       "4  JOB PURPOSE:    The Account Director is respon...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "job_tag = ['part-time-job', 'full-time-job', 'hourly-wage', 'salary', 'associate-needed', 'bs-degree-needed', 'ms-or-phd-needed', 'licence-needed', '1-year-experience-needed', '2-4-years-experience-needed', '5-plus-years-experience-needed', 'supervising-job']\n",
    "new_column = ['part_time_job', 'full_time_job', 'hourly_wage', 'salary', 'associate_needed', 'bs_degree_needed', 'ms_or_phd_needed', 'licence_needed', '1_year_experience_needed', '2_4_years_experience_needed', '5_plus_years_experience_needed', 'supervising_job']\n",
    "\n",
    "# preprocess tags\n",
    "part_time_job = df.tags.str.extract('(?P<part_time_job>part-time-job)').fillna(0)\n",
    "part_time_job.loc[part_time_job.str.contains('part-time-job', na=False)] = 1\n",
    "full_time_job = df.tags.str.extract('(?P<full_time_job>full-time-job)').fillna(0)\n",
    "full_time_job.loc[full_time_job.str.contains('full-time-job', na=False)] = 1\n",
    "hourly_wage = df.tags.str.extract('(?P<hourly_wage>hourly-wage)').fillna(0)\n",
    "hourly_wage.loc[hourly_wage.str.contains('hourly-wage', na=False)] = 1\n",
    "salary = df.tags.str.extract('(?P<salary>salary)').fillna(0)\n",
    "salary.loc[salary.str.contains('salary', na=False)] = 1\n",
    "associate_needed = df.tags.str.extract('(?P<associate_needed>associate-needed)').fillna(0)\n",
    "associate_needed.loc[associate_needed.str.contains('associate-needed', na=False)] = 1\n",
    "bs_degree_needed = df.tags.str.extract('(?P<bs_degree_needed>bs-degree-needed)').fillna(0)\n",
    "bs_degree_needed.loc[bs_degree_needed.str.contains('bs-degree-needed', na=False)] = 1\n",
    "ms_or_phd_needed = df.tags.str.extract('(?P<ms_or_phd_needed>ms-or-phd-needed)').fillna(0)\n",
    "ms_or_phd_needed.loc[ms_or_phd_needed.str.contains('ms-or-phd-needed', na=False)] = 1\n",
    "licence_needed = df.tags.str.extract('(?P<licence_needed>licence-needed)').fillna(0)\n",
    "licence_needed.loc[licence_needed.str.contains('licence-needed', na=False)] = 1\n",
    "one_year_experience_needed = df.tags.str.extract('(?P<one_year_experience_needed>1-year-experience-needed)').fillna(0)\n",
    "one_year_experience_needed.loc[one_year_experience_needed.str.contains('1-year-experience-needed', na=False)] = 1\n",
    "two_four_years_experience_needed = df.tags.str.extract('(?P<two_four_years_experience_needed>2-4-years-experience-needed)').fillna(0)\n",
    "two_four_years_experience_needed.loc[two_four_years_experience_needed.str.contains('2-4-years-experience-needed', na=False)] = 1\n",
    "five_plus_years_experience_needed = df.tags.str.extract('(?P<five_plus_years_experience_needed>5-plus-years-experience-needed)').fillna(0)\n",
    "five_plus_years_experience_needed.loc[five_plus_years_experience_needed.str.contains('5-plus-years-experience-needed', na=False)] = 1\n",
    "supervising_job = df.tags.str.extract('(?P<supervising_job>supervising-job)').fillna(0)\n",
    "supervising_job.loc[supervising_job.str.contains('supervising-job', na=False)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocess description\n",
    "def preprocess(sentence, language='english', stopword=True, nonascii=True, punctuation='+'):\n",
    "    \"\"\"Preprocess String, remove punctuation and delete stopwords\n",
    "    Parameters : sentence\n",
    "    Returns : sentence\n",
    "    \"\"\"\n",
    "    import re\n",
    "    import string\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    if nonascii == True:\n",
    "        sentence = re.sub(\"[^\\x00-\\x7F]+\\ *(?:[^\\x00-\\x7F]| )*\", \"\", sentence, flags=re.UNICODE)\n",
    "    \n",
    "    if punctuation is not None:\n",
    "        table = string.maketrans(\"\", \"\")\n",
    "        remove = string.punctuation  # delete all punctuation       \n",
    "        \n",
    "        if punctuation != 'all':             \n",
    "            for i in punctuation:  # list all punctuation that don't want to deleted\n",
    "                remove = remove.replace(i, '')\n",
    "\n",
    "        # delete punctuation\n",
    "        sentence = sentence.translate(table, remove)   \n",
    "\n",
    "    words = re.split(r'\\s', sentence)  # delete empty char from list\n",
    "        \n",
    "    # stopword\n",
    "    if stopword == True:\n",
    "        if language == 'indonesia':\n",
    "            basepath = os.path.dirname(__file__)\n",
    "            rel_path = \"stopword.txt\"  # get stopwords\n",
    "            filepath = os.path.abspath(os.path.join(basepath, rel_path))\n",
    "            f = open(filepath, \"r\")\n",
    "            stopwords = [line.rstrip('\\n') for line in f]\n",
    "            words = filter(lambda x: x not in stopwords, words)\n",
    "\n",
    "        elif language == 'english':\n",
    "            stopwords = stopwords.words('english')\n",
    "            words = filter(lambda x: x not in stopwords, words)\n",
    "\n",
    "    sentence = ' '.join(words).lower()\n",
    "    return sentence\n",
    "\n",
    "\n",
    "preproce_desc = df['description'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean \n",
    "clean_df = pd.concat([part_time_job, full_time_job, hourly_wage, salary, associate_needed, bs_degree_needed, ms_or_phd_needed, licence_needed, one_year_experience_needed, two_four_years_experience_needed, five_plus_years_experience_needed, supervising_job], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 1, 1],\n",
       "       [0, 1, 0, ..., 1, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0]], dtype=object)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = clean_df.as_matrix()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_doc = list(preproce_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_of_words = []\n",
    "for i in list_of_doc:\n",
    "    list_of_words.append(i.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "from gensim.models import Word2Vec, Doc2Vec, doc2vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "docs = []\n",
    "for i, item in enumerate(list_of_words):\n",
    "    docs.append(LabeledSentence(item, ['SENT_{}'.format(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = doc2vec.Doc2Vec(docs, size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MakeDoc2Vec(object):\n",
    "    def __init__(self, list_of_words, size=100, window=5, min_count=5):\n",
    "        \"\"\"Create Doc2Vec model\n",
    "\n",
    "        Args:\n",
    "            list_of_words (list of list of words): Description\n",
    "            size (int, optional): doc2vec vector size\n",
    "            window (int, optional): doc2vec window size\n",
    "            min_count (int, optional):\n",
    "        \"\"\"\n",
    "        from gensim.models import doc2vec\n",
    "        from gensim.models.doc2vec import LabeledSentence                \n",
    "        \n",
    "        index = 1\n",
    "        list_of_docs = []\n",
    "        for i in list_of_words:\n",
    "            list_of_docs.append(LabeledSentence(i, ['doc_{}'.format(index)]))\n",
    "            index += 1        \n",
    "        self.model = doc2vec.Doc2Vec(list_of_docs, size=size, window=window, min_count=min_count, workers=4)\n",
    "        self.doc_len = len(list_of_words)\n",
    "    \n",
    "    def to_array(self):\n",
    "        \"\"\"Convert model into data features\n",
    "\n",
    "        Returns:\n",
    "            np.array: array of vectors (as feature)\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "\n",
    "        matrix = [self.model.docvecs[i] for i in range(self.doc_len)]\n",
    "        return np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02941223,  0.04770463, -0.09342659, ..., -0.01542124,\n",
       "        -0.11207563, -0.12267005],\n",
       "       [ 0.14190421, -0.0271364 , -0.16233172, ...,  0.17671724,\n",
       "        -0.19153301, -0.11766765],\n",
       "       [-0.04485836, -0.03472598, -0.03271582, ..., -0.01299365,\n",
       "        -0.11186631, -0.02899892],\n",
       "       ..., \n",
       "       [-0.01796105, -0.0170827 , -0.08131669, ...,  0.05733385,\n",
       "        -0.09946719, -0.03017049],\n",
       "       [ 0.00170175,  0.05902698, -0.06728472, ...,  0.0423625 ,\n",
       "        -0.06935985, -0.06601907],\n",
       "       [ 0.22227433,  0.12489616,  0.06726623, ...,  0.04060264,\n",
       "        -0.00189521,  0.04933609]], dtype=float32)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = MakeDoc2Vec(list_of_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02941223,  0.04770463, -0.09342659, ..., -0.01542124,\n",
       "        -0.11207563, -0.12267005],\n",
       "       [ 0.14190421, -0.0271364 , -0.16233172, ...,  0.17671724,\n",
       "        -0.19153301, -0.11766765],\n",
       "       [-0.04485836, -0.03472598, -0.03271582, ..., -0.01299365,\n",
       "        -0.11186631, -0.02899892],\n",
       "       ..., \n",
       "       [-0.01796105, -0.0170827 , -0.08131669, ...,  0.05733385,\n",
       "        -0.09946719, -0.03017049],\n",
       "       [ 0.00170175,  0.05902698, -0.06728472, ...,  0.0423625 ,\n",
       "        -0.06935985, -0.06601907],\n",
       "       [ 0.22227433,  0.12489616,  0.06726623, ...,  0.04060264,\n",
       "        -0.00189521,  0.04933609]], dtype=float32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.to_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matrix = np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "y = y.astype(int)\n",
    "classif = OneVsRestClassifier(SVC(kernel='linear'))\n",
    "classif.fit(matrix, y)\n",
    "# X, Y = make_multilabel_classification(n_classes=2, n_labels=1, allow_unlabeled=False, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.array(matrix[100])\n",
    "classif.predict([matrix[100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.02717997,  0.28184655,  0.02584346, -0.18644425,  0.26854518,\n",
       "         0.20918666, -0.07650733, -0.3947601 ,  0.06591658, -0.20024657,\n",
       "        -0.20843254, -0.06139557, -0.03878127,  0.01103263,  0.27453291,\n",
       "        -0.08868945, -0.14218737,  0.03249107,  0.02612321, -0.29735258,\n",
       "         0.02626696, -0.31969944,  0.27245292, -0.15001689,  0.05682455,\n",
       "         0.00762475,  0.01729175, -0.33488351,  0.07656159,  0.12478656,\n",
       "        -0.03130287, -0.144877  ,  0.10193414, -0.04916345,  0.23277928,\n",
       "        -0.05448294,  0.19289318,  0.2469434 , -0.07819665, -0.01994963,\n",
       "        -0.00246969,  0.01988657, -0.19254416,  0.2605553 ,  0.0575675 ,\n",
       "         0.14912358, -0.07724001,  0.23357044,  0.23653488, -0.23393616,\n",
       "        -0.15722269,  0.05987019,  0.06734546,  0.08714455, -0.01286892,\n",
       "        -0.05806176,  0.03630435,  0.10261052, -0.17152894, -0.04822818,\n",
       "        -0.06727435, -0.52981895, -0.05492345,  0.05497908, -0.23903482,\n",
       "         0.01909414, -0.25160712,  0.06085788, -0.08038   ,  0.00392221,\n",
       "        -0.11486973, -0.15690123, -0.12975872, -0.31143871,  0.09944622,\n",
       "         0.22361387,  0.04728353,  0.19041988,  0.0543741 , -0.08384506,\n",
       "        -0.2504729 , -0.29060382,  0.27468249,  0.03479307,  0.07982693,\n",
       "        -0.02347434,  0.24368322, -0.26908165, -0.24467748,  0.07807815,\n",
       "         0.21210803,  0.18139759, -0.1247676 ,  0.08907263,  0.14489435,\n",
       "         0.33417517,  0.07180945,  0.22956358, -0.14657639, -0.03479825], dtype=float32)]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[matrix[100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = [\"The sky is is blue.\", \"The sun in the sky is bright.\"]\n",
    "test_set = (\"The sun in the sky is bright.\",\n",
    "    \"We can see the shining sun, the bright sun.\")\n",
    "\n",
    "def get_vector(dataset):\n",
    "    vectorizer = CountVectorizer(stop_words='english', min_df=1, max_df=.5, ngram_range=(1,2))\n",
    "    vectorizer.fit(dataset)\n",
    "    print vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "-0.405465108108\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def tf(word, blob):\n",
    "    '''\n",
    "    tf computes \"term frequency\" which is the number of times a word appears in a document blob, \n",
    "    normalized by dividing by the total number of words in document. \n",
    "    to compute tf breaking up the text into words and getting the word counts.\n",
    "    '''\n",
    "\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "def idf(word, bloblist):    \n",
    "    '''\n",
    "    idf computes \"inverse document frequency\" which measures how common a word is among all documents. \n",
    "    The more common a word is, the lower its idf. \n",
    "    We take the ratio of the total number of documents to the number of documents containing word, then take the log of that. \n",
    "    a word that occurs 10 times more than another isn’t 10 times more important than it, that’s why tf-idf uses the logarithmic scale to do that.\n",
    "    Add 1 to the divisor to prevent division by zero.\n",
    "    '''    \n",
    "    return math.log(float(len(bloblist)) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "doc1 = tb(train_set[0])\n",
    "doc2 = tb(train_set[0])\n",
    "\n",
    "print(idf('sky', [doc1, doc2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
